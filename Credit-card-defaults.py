# -*- coding: utf-8 -*-
"""ML-Homework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16w8yBDDOhXzUTAHoZT5CoQUuD5-IYjoD
"""

!pip install pandas

# Import / install relevant Python packages
import numpy as np
from numpy import where
import pandas as pd
from pandas import set_option
import datetime as dt
import statsmodels.api as sm
from scipy.stats import (randint, loguniform)

from collections import Counter

from sklearn.datasets import make_blobs
from sklearn.datasets import make_multilabel_classification

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import (RFE, SelectFromModel)
from sklearn.model_selection import (KFold, GridSearchCV, RandomizedSearchCV, cross_val_score)
from sklearn import metrics
from sklearn.metrics import (classification_report, confusion_matrix)
from sklearn.svm import (SVC, LinearSVC)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import (DecisionTreeClassifier, plot_tree)
from sklearn.ensemble import RandomForestClassifier

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
plt.style.use('ggplot')
import seaborn as sns

pip install --upgrade xlrd

"""### **1**"""

# Import data 
data = pd.read_excel('default of credit card clients.xls')
data.head()

"we will use the describe and info fuction to get the initail feel for the data"

data.describe()

data.info()

"we observe that there are no Nan"

#renaming and droping data to be congruent with our terminology#
data.drop('Unnamed: 0', axis = 1, inplace =True) 
data.info()

data.rename(columns={'Y':'DEFAULT'}, inplace=True)

data.head()

#checking for descrepencies#
# EDA: Print unique values for selected features
print('Gender (1 = male; 2 = female):')
print(data['X2'].unique())
print('-'*100)
print('Education (1 = graduate school; 2 = university; 3 = high school; 4 = others):')
print(data['X3'].unique())
print('-'*100)
print('Marital status (1 = married; 2 = single; 3 = others):')
print(data['X4'].unique())
print('-'*100)
print('Payment status (-1 = pay duly; K = payment delay for K months, for K=1,2,...,8; 9 = payment delay for nine months and above):')
print(data['X6'].unique())
print(data['X7'].unique())
print(data['X8'].unique())
print(data['X9'].unique())
print(data['X10'].unique())
print(data['X11'].unique())
print('-'*100)

#we have a discrepency in the data what does 0 mean for education,0 and -2 for pay and marital status#

# we will try to corect the data for education and marriage
data['X3']=np.where(data['X3'] == 5, 4, data['X3'])
data['X3']=np.where(data['X3'] == 6, 4, data['X3'])
data['X3']=np.where(data['X3'] == 0, 4, data['X3'])
data['X4']=np.where(data['X4'] == 0, 3, data['X4'])

data_of_variable=data.iloc[1: , :].astype(int)
data_of_variable

corr = data_of_variable.corr() 
fig, ax = plt.subplots(figsize=(24,24))
sns.heatmap(corr, cbar = True,  square = True, annot = False, fmt= '.1f', 
            xticklabels= True, yticklabels= True, cmap='coolwarm', linewidths=.5, ax=ax)
plt.title('Correlation Matrix Heatmap', size=24)
plt.show()

data_of_variable.hist(figsize=(36, 24), bins=50, edgecolor='black')
plt.subplots_adjust(hspace=0.5, wspace=0.5)
plt.show()
#we observe that there is a lot of grouping of the data#

#defining the variables#
X = data_of_variable.drop('DEFAULT', axis = 1, inplace = False)
y = data_of_variable['DEFAULT']

## EDA: Box plot of all variables
plt.figure(figsize=(24,12))
ax = data_of_variable.boxplot(data_of_variable.columns.name, rot=90)
outliers = dict(markerfacecolor='b', marker='p')
ax= data_of_variable.boxplot(data_of_variable.columns.name, rot=90, flierprops=outliers)
plt.xticks(size=24)
plt.title('Box Plot of All Variables', size=24)
ax.set_ylim([-5000,100000])
plt.show()

#In the data the variables are of diffrent scale we will proceed to standardize them.#
#In addtion the data has a lot of outliers. #

X_stand = (X - X.mean()) / (X.std())

scaled = pd.concat([y,X_stand.iloc[:,:]],axis=1)
scaled = pd.melt(scaled, id_vars='DEFAULT', var_name='Features', value_name='Value')

plt.figure(figsize=(24,12))
sns.set_context('notebook', font_scale=2)
ax = sns.violinplot(y='Value', x='Features', hue='DEFAULT', data=scaled, split=True, 
                    inner='quart', palette='viridis')
ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)
plt.legend(loc=4, title= 'Default', facecolor='white')
plt.ylim([-3,3])
plt.title('Standardized Variables', size=14)
plt.show()

X_scaled_train, X_scaled_test, y_scaled_train, y_scaled_test = train_test_split(X_stand, y, test_size=0.333, 
                                                                                stratify=y, random_state=42)

#we will try to use the pricipal componets to transform the data(because of correlation in the data),#
#we will also continue to work with the scaled data for comparison i  the end#

from sklearn.decomposition import PCA
pca = PCA(n_components = 5)#lets try with 5 components
X_train_pca=pca.fit_transform(X_scaled_train)
X_test_pca=pca.fit_transform(X_scaled_test)

# Recursive feature selection (rough estimate)
n_features = 5
logit = LogisticRegression()
RFE_scaled = RFE(estimator=logit,n_features_to_select=n_features,step=1)
fit_scaled  = RFE_scaled.fit(X_scaled_train, y_scaled_train)

print('Number of features:', fit_scaled.n_features_)
print('Selected features:', fit_scaled.support_)
print('Ranking of model features:', fit_scaled.ranking_)

score_scaled = RFE_scaled.score(X_stand,y)
print('Standardized model score with selected features is: %f (%f)' % (score_scaled.mean(), score_scaled.std()))

feature_names = np.array(X.columns)
print('Most important features (RFE): %s'% feature_names[RFE_scaled.support_])

#now for pca#
logit = LogisticRegression()
logit.fit(X_train_pca,y_scaled_train)
logit.coef_

logit.score(X_train_pca,y_scaled_train)

X_scaled_train[['X6','X8','X12','X18','X19']]

#not much of a diffrence we will try to see who predicts better #

# searching for optimal hyper-parameters for the train sample
n_cv_logit = 5

# Define the maximum number of iterations
n_max_iter = 500

# Create an instance of the class LogisticRegression
logit_model = LogisticRegression(max_iter=n_max_iter)

# Define the search space for the hyperparameters
search_space = dict()
search_space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']
search_space['solver']  = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
search_space['C']       = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]

# Create an instance of the class GridSearchCV with desired characteristics
logit_grid_search_cv_original = GridSearchCV(estimator=logit_model, 
                                             param_grid=search_space, 
                                             scoring='accuracy', 
                                             cv=n_cv_logit, 
                                             n_jobs=-1)

# Run the search algorithm and summarize resuts 
logit_grid_search_cv_original.fit(X_scaled_train[['X6','X8','X12','X18','X19']], y_scaled_train)

print(f'Best score: {logit_grid_search_cv_original.best_score_:.4f}')
print('Best hyperparameters: %s' % logit_grid_search_cv_original.best_params_)

# searching for optimal hyper-parameters for the train sample in the case for pca
n_cv_logit = 5

# Define the maximum number of iterations
n_max_iter = 500

# Create an instance of the class LogisticRegression
logit_model = LogisticRegression(max_iter=n_max_iter)

# Define the search space for the hyperparameters
search_space = dict()
search_space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']
search_space['solver']  = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
search_space['C']       = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]

# Create an instance of the class GridSearchCV with desired characteristics
logit_grid_search_cv_original = GridSearchCV(estimator=logit_model, 
                                             param_grid=search_space, 
                                             scoring='accuracy', 
                                             cv=n_cv_logit, 
                                             n_jobs=-1)

# Run the search algorithm and summarize resuts 
logit_grid_search_cv_original.fit(X_train_pca, y_scaled_train)

print(f'Best score: {logit_grid_search_cv_original.best_score_:.4f}')
print('Best hyperparameters: %s' % logit_grid_search_cv_original.best_params_)

#it seems that the standard approach with stadardized features seems to work better in insample fitting #
#we will proceed to prediction#

logit_ccd_original = LogisticRegression(penalty='none', 
                                        solver='newton-cg', 
                                        C= 1e-05)

# Fit the model
logit_ccd_original.fit(X_scaled_train[['X6','X8','X12','X18','X19']], y_scaled_train)
print('Logistic regression coefficients:')
print(logit_ccd_original.coef_.T)
print(100*'-')

# Predict outcomes in the training and testing subsamples and calculate accuracy
y_pred_train = logit_ccd_original.predict(X_scaled_train[['X6','X8','X12','X18','X19']])


print(f'Accuracy (train): {metrics.accuracy_score(y_pred_train,y_scaled_train):.4f}')
print(100*'-')

y_pred_test  = logit_ccd_original.predict(X_scaled_test[['X6','X8','X12','X18','X19']])


print(f'Accuracy (test): {metrics.accuracy_score(y_pred_test,y_scaled_test):.4f}')
print(100*'-')

## Calculate and display cross-validation scores for the training and testing subsamples
logit_cv_scores = cross_val_score(logit_ccd_original, X_scaled_train, y_scaled_train, cv=n_cv_logit)
print(f'Average CV score (+/- one standard deviation): {logit_cv_scores.mean():.4f} +/- {logit_cv_scores.std():.4f}')

print(classification_report(y_scaled_train, y_pred_train))
print(classification_report(y_scaled_test, y_pred_test))

logit_ccd_original_pca = LogisticRegression(penalty='l2', 
                                        solver='liblinear', 
                                        C=0.0001)

# Fit the model
logit_ccd_original_pca.fit(X_train_pca, y_scaled_train)
print('Logistic regression coefficients:')
print(logit_ccd_original_pca.coef_.T)
print(100*'-')

# Predict outcomes in the training and testing subsamples and calculate accuracy
y_pred_train_pca = logit_ccd_original_pca.predict(X_train_pca)


print(f'Accuracy (train): {metrics.accuracy_score(y_pred_train,y_scaled_train):.4f}')
print(100*'-')

y_pred_test_pca  = logit_ccd_original_pca.predict(X_test_pca)


print(f'Accuracy (test): {metrics.accuracy_score(y_pred_test,y_scaled_test):.4f}')
print(100*'-')

## Calculate and display cross-validation scores for the training and testing subsamples
logit_cv_scores = cross_val_score(logit_ccd_original_pca, X_scaled_train, y_scaled_train, cv=n_cv_logit)
print(f'Average CV score (+/- one standard deviation): {logit_cv_scores.mean():.4f} +/- {logit_cv_scores.std():.4f}')

print(classification_report(y_scaled_train, y_pred_train))
print(classification_report(y_scaled_test, y_pred_test))

#it seems that the standardized approach seems to give the same accuracy as the PCA transformation#
#we will proceed to the confusion matrix
# Plot the confusion matrix (training subsample)
plt.figure(figsize=(24,16))
confusion_train = confusion_matrix(y_scaled_train,y_pred_train)
sns.heatmap(confusion_train, annot=True, annot_kws={'size':24}, cmap='coolwarm', fmt='d', 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'])
plt.ylabel('True label', size=24)
plt.xlabel('Predicted label', size=24)
plt.title('Confusion Matrix for Logistic Regression (Original Features): Training Subsample', size=24)
plt.show()#

# Plot the confusion matrix (testing subsample)
plt.figure(figsize=(24,16))
confusion_test = confusion_matrix(y_scaled_test,y_pred_test)
sns.heatmap(confusion_test, annot=True, annot_kws={'size':24}, cmap='coolwarm', fmt='d', 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'])
plt.ylabel('True label', size=24)
plt.xlabel('Predicted label', size=24)
plt.title('Confusion Matrix for Logistic Regression (Original Features): Testing Subsample', size=24)
plt.show()

#lets check out the pca example#
# Plot the confusion matrix (training subsample)
plt.figure(figsize=(24,16))
confusion_train = confusion_matrix(y_scaled_train,y_pred_train_pca)
sns.heatmap(confusion_train, annot=True, annot_kws={'size':24}, cmap='coolwarm', fmt='d', 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'])
plt.ylabel('True label', size=24)
plt.xlabel('Predicted label', size=24)
plt.title('Confusion Matrix for Logistic Regression (PCA): Training Subsample', size=24)
plt.show()#

# Plot the confusion matrix (testing subsample)
plt.figure(figsize=(24,16))
confusion_test = confusion_matrix(y_scaled_test,y_pred_test_pca)
sns.heatmap(confusion_test, annot=True, annot_kws={'size':24}, cmap='coolwarm', fmt='d', 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'])
plt.ylabel('True label', size=24)
plt.xlabel('Predicted label', size=24)
plt.title('Confusion Matrix for Logistic Regression (PCA): Testing Subsample', size=24)
plt.show()

#the PCA made less mistakes in pred non default and true default field and more accuratly predicted the true defaults.#
#The reason why it was over all worse is of the remaining two fields it can be argued that the PCA is a more conservative approach in this case (and is safer bet),#
#but would probally lead to the bank giving less loans and not making hypotheticly as much money #
#furhter analysis would be requaired to deduce the optimal method#

X_scaled_train[['X6','X8','X12','X18','X19']]

summary = pd.DataFrame(columns=['Model name', 'AUC Train', 'AUC Test', 'F1 Train', 'F1 Test'])
summary

#ROC and AUC for standarized approach
y_pred_proba_train      = logit_ccd_original.predict_proba(X_scaled_train[['X6','X8','X12','X18','X19']])[::,1]
fpr_train, tpr_train, threshold_train = metrics.roc_curve(y_scaled_train, y_pred_proba_train)
auc_train               = metrics.roc_auc_score(y_scaled_train, y_pred_proba_train)

y_pred_proba_test      = logit_ccd_original.predict_proba(X_scaled_test[['X6','X8','X12','X18','X19']])[::,1]
fpr_test, tpr_test, threshold_test  = metrics.roc_curve(y_scaled_test, y_pred_proba_test)
auc_test               = metrics.roc_auc_score(y_scaled_test, y_pred_proba_test)

# Plot the ROC curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [0, 1], 'b--')
plt.plot(fpr_train, tpr_train, label='Logistic Regression (Training), auc='+str(round(auc_train,6)))
plt.plot(fpr_test, tpr_test, label='Logistic Regression (Testing), auc='+str(round(auc_test,6)))
plt.legend(loc=4, facecolor='white')
plt.xlabel('False Positive Rate', size=24)
plt.ylabel('True Positive Rate', size=24)
plt.title('ROC: Original Features', size=24)
plt.show()

model_summary_lr = {'Model name': 'Logistic Regression', 'AUC Train': auc_train, 'AUC Test': auc_test}

#ROC and AUC for pca
y_pred_proba_train_pca     = logit_ccd_original_pca.predict_proba(X_train_pca)[::,1]
fpr_train_pca, tpr_train_pca, threshold_train_pca = metrics.roc_curve(y_scaled_train, y_pred_proba_train_pca)
auc_train_pca               = metrics.roc_auc_score(y_scaled_train, y_pred_proba_train_pca)

y_pred_proba_test_pca      = logit_ccd_original_pca.predict_proba(X_test_pca)[::,1]
fpr_test_pca, tpr_test_pca, threshold_test_pca  = metrics.roc_curve(y_scaled_test, y_pred_proba_test_pca)
auc_test_pca              = metrics.roc_auc_score(y_scaled_test, y_pred_proba_test)

# Plot the ROC curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [0, 1], 'b--')
plt.plot(fpr_train_pca, tpr_train_pca, label='Logistic Regression (Training) PCA, auc='+str(round(auc_train_pca,6)))
plt.plot(fpr_test_pca, tpr_test_pca, label='Logistic Regression (Testing) PCA, auc='+str(round(auc_test_pca,6)))
plt.legend(loc=4, facecolor='white')
plt.xlabel('False Positive Rate', size=24)
plt.ylabel('True Positive Rate', size=24)
plt.title('ROC: PCA', size=24)
plt.show()

model_summary_lr_pca = {'Model name': 'Logistic Regression PCA', 'AUC Train': auc_train, 'AUC Test': auc_test, }

#finally when it comes to AUC and ROC the standardized method is better#

#finding optimal probability for standardized approach#
# Calculate the geometric mean for each threshold for the training and testing subsamples
vec_gmeans_train = np.sqrt(tpr_train * (1-fpr_train))
vec_gmeans_test  = np.sqrt(tpr_test * (1-fpr_test))

# locate the index of the largest g-mean
ix_train = np.argmax(vec_gmeans_train)
ix_test  = np.argmax(vec_gmeans_test)

# Summarize the results
print('Training (Original features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_train[ix_train], vec_gmeans_train[ix_train]))
print(100*'-')
print('Testing (Original features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_test[ix_test], vec_gmeans_test[ix_test]))
print(100*'-')

#finding optimal probability for standardized approach#
# Calculate the geometric mean for each threshold for the training and testing subsamples
vec_gmeans_train = np.sqrt(tpr_train_pca * (1-fpr_train_pca))
vec_gmeans_test  = np.sqrt(tpr_test_pca * (1-fpr_test_pca))

# locate the index of the largest g-mean
ix_train = np.argmax(vec_gmeans_train)
ix_test  = np.argmax(vec_gmeans_test)

# Summarize the results
print('Training (PCA):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_train_pca[ix_train], vec_gmeans_train[ix_train]))
print(100*'-')
print('Testing ( PCA):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_test_pca[ix_test], vec_gmeans_test[ix_test]))
print(100*'-')

#both approaches gives us the same prob treshhold

# Calculate the precision and recall for different values of threshold in the training and testing subsamples standardized approach
precision_train, recall_train, threshold_train = metrics.precision_recall_curve(y_scaled_train, y_pred_proba_train)
average_precision_train                        = metrics.average_precision_score(y_scaled_train, y_pred_train)

precision_test, recall_test, threshold_test    = metrics.precision_recall_curve(y_scaled_test, y_pred_proba_test)
average_precision_test                         = metrics.average_precision_score(y_scaled_test, y_pred_test)

# Calculate the F1 score
vec_f1_train = 2 * precision_train * recall_train / (precision_train + recall_train)
vec_f1_test  = 2 * precision_test * recall_test / (precision_test + recall_test)

# locate the index of the largest F1 score
ix_f1_train = np.argmax(vec_f1_train)
ix_f1_test  = np.argmax(vec_f1_test)

# Summarize the results
print('Training (Original features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_train[ix_f1_train], vec_f1_train[ix_f1_train]))
print(100*'-')
print('Testing (Original features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_test[ix_f1_test], vec_f1_test[ix_f1_test]))
print(100*'-')

# Calculate benchmark values for the training and testing subsamples
nopower_scaled_train = len(y_scaled_train[y_scaled_train==1]) / len(y_scaled_train)
nopower_scaled_test  = len(y_scaled_test[y_scaled_test==1]) / len(y_scaled_test)

# Plot the PR curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [nopower_scaled_train, nopower_scaled_train], 'b--')
plt.plot([0, 1], [nopower_scaled_test, nopower_scaled_test], 'k--')
plt.plot(recall_train, precision_train, 'r-', label='Logistic Regression (Training), average precision='+str(round(average_precision_train,6)))
plt.plot(recall_test, precision_test, 'g-', label='Logistic Regression (Testing), average_precision='+str(round(average_precision_test,6)))
plt.scatter(recall_train[ix_f1_train], precision_train[ix_f1_train], marker='o', color='red', s =150)
plt.scatter(recall_test[ix_f1_test], precision_test[ix_f1_test], marker='o', color='green', s=150)
plt.legend(loc=1, facecolor='white')
plt.xlabel('Recall', size=24)
plt.ylabel('Precision', size=24)
plt.title('Logistic Regression PR Curve: Original Features', size=24)
plt.show()

model_summary_lr['F1 Train'] = vec_f1_train[ix_f1_train]
model_summary_lr['F1 Test'] = vec_f1_test[ix_f1_test]
summary = summary.append(model_summary_lr, ignore_index=True)

# Calculate the precision and recall for different values of threshold in the training and testing subsamples  PCA approach
precision_train_pca, recall_train_pca, threshold_train_pca = metrics.precision_recall_curve(y_scaled_train, y_pred_proba_train_pca)
average_precision_train_pca                       = metrics.average_precision_score(y_scaled_train, y_pred_train)

precision_test_pca, recall_test_pca, threshold_test_pca    = metrics.precision_recall_curve(y_scaled_test, y_pred_proba_test_pca)
average_precision_test_pca                        = metrics.average_precision_score(y_scaled_test, y_pred_test)

# Calculate the F1 score
vec_f1_train = 2 * precision_train_pca * recall_train_pca / (precision_train_pca + recall_train_pca)
vec_f1_test  = 2 * precision_test_pca * recall_test_pca / (precision_test_pca+ recall_test_pca)

# locate the index of the largest F1 score
ix_f1_train = np.argmax(vec_f1_train)
ix_f1_test  = np.argmax(vec_f1_test)

# Summarize the results
print('Training (PCA):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_train_pca[ix_f1_train], vec_f1_train[ix_f1_train]))
print(100*'-')
print('Testing (PCA ):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_test_pca[ix_f1_test], vec_f1_test[ix_f1_test]))
print(100*'-')

# Calculate benchmark values for the training and testing subsamples
nopower_scaled_train = len(y_scaled_train[y_scaled_train==1]) / len(y_scaled_train)
nopower_scaled_test  = len(y_scaled_test[y_scaled_test==1]) / len(y_scaled_test)

# Plot the PR curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [nopower_scaled_train, nopower_scaled_train], 'b--')
plt.plot([0, 1], [nopower_scaled_test, nopower_scaled_test], 'k--')
plt.plot(recall_train_pca, precision_train_pca, 'r-', label='Logistic Regression (Training), average precision='+str(round(average_precision_train_pca,6)))
plt.plot(recall_test_pca, precision_test_pca, 'g-', label='Logistic Regression (Testing), average_precision='+str(round(average_precision_test_pca,6)))
plt.scatter(recall_train_pca[ix_f1_train], precision_train_pca[ix_f1_train], marker='o', color='red', s =150)
plt.scatter(recall_test_pca[ix_f1_test], precision_test_pca[ix_f1_test], marker='o', color='green', s=150)
plt.legend(loc=1, facecolor='white')
plt.xlabel('Recall', size=24)
plt.ylabel('Precision', size=24)
plt.title('Logistic Regression PR Curve: PCA', size=24)
plt.show()

model_summary_lr_pca['F1 Train'] = vec_f1_train[ix_f1_train]
model_summary_lr_pca['F1 Test'] = vec_f1_test[ix_f1_test]
summary = summary.append(model_summary_lr_pca, ignore_index=True)

#the PCA has a worse f1 score#

"""### **2**


"""

#we will start our anallysis with Linear SVC#

# Linear SVC cross-validation approach
n_cv_lsvc = 5

# Define the maximum number of iterations
n_max_iter = 2000

# Create an instance of the class LinearSVC
lsvc_model = LinearSVC(max_iter=n_max_iter)

# Define the search space for the hyperparameters
search_space = dict()
search_space['penalty'] = ['l1', 'l2']
search_space['loss']    = ['hinge', 'squared_hinge']
search_space['C']       = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]

# Create an instance of the class GridSearchCV with desired characteristics
lsvc_grid_search_cv_scaled = GridSearchCV(estimator=lsvc_model, 
                                          param_grid=search_space, 
                                          scoring='accuracy', 
                                          cv=n_cv_lsvc, 
                                          n_jobs=-1)

# Run the search algorithm and summarize resuts 
lsvc_grid_search_cv_scaled.fit(X_scaled_train, y_scaled_train)

print(f'Best score: {lsvc_grid_search_cv_scaled.best_score_:.4f}')
print('Best hyperparameters: %s' % lsvc_grid_search_cv_scaled.best_params_)

# Create an instance of the class LinearSVC with specific selection of hyperparameters
lsvc_ccd_scaled = LinearSVC(penalty=lsvc_grid_search_cv_scaled.best_params_['penalty'], 
                            loss=lsvc_grid_search_cv_scaled.best_params_['loss'], 
                            C=lsvc_grid_search_cv_scaled.best_params_['C'], 
                            max_iter=n_max_iter)

# Fit the model
lsvc_ccd_scaled.fit(X_scaled_train, y_scaled_train)
print(lsvc_ccd_scaled.coef_.T)
print(100*'-')

# Predict outcomes in the training and testing subsamples and calculate accuracy
y_scaled_pred_train = lsvc_ccd_scaled.predict(X_scaled_train)
print(f'Accuracy: {metrics.accuracy_score(y_scaled_pred_train,y_scaled_train):.4f}')

y_scaled_pred_test = lsvc_ccd_scaled.predict(X_scaled_test)
print(f'Accuracy: {metrics.accuracy_score(y_scaled_pred_test,y_scaled_test):.4f}')

# Calculate and display cross-validation scores for the training and testing subsamples
lsvc_cv_scores_scaled =cross_val_score(lsvc_ccd_scaled, X_scaled_train, y_scaled_train, cv=n_cv_lsvc)
print(f'Average CV score (+/- one standard deviation): {lsvc_cv_scores_scaled.mean():.4f} +/- {lsvc_cv_scores_scaled.std():.4f}')

print(classification_report(y_scaled_train, y_scaled_pred_train))
print(classification_report(y_scaled_test, y_scaled_pred_test))

# Estimate probabilities, and calcualate relevant ROC/AUC scores 
lsvc_calibrated_classifier_cv_ccd_scaled = CalibratedClassifierCV(lsvc_ccd_scaled) 
lsvc_calibrated_classifier_cv_ccd_scaled.fit(X_scaled_train, y_scaled_train)
y_scaled_pred_proba_train = lsvc_calibrated_classifier_cv_ccd_scaled.predict_proba(X_scaled_train)[::,1]

fpr_scaled_train, tpr_scaled_train, threshold_scaled_train  = metrics.roc_curve(y_scaled_train, y_scaled_pred_proba_train)
auc_scaled_train          = metrics.roc_auc_score(y_scaled_train, y_scaled_pred_proba_train)

lsvc_calibrated_classifier_cv_ccd_scaled.fit(X_scaled_test, y_scaled_test)
y_scaled_pred_proba_test  = lsvc_calibrated_classifier_cv_ccd_scaled.predict_proba(X_scaled_test)[::,1]
fpr_scaled_test, tpr_scaled_test, threshold_scaled_test = metrics.roc_curve(y_scaled_test, y_scaled_pred_proba_test)
auc_scaled_test           = metrics.roc_auc_score(y_scaled_test, y_scaled_pred_proba_test)

# Calculate the geometric mean for each threshold for the training and testing subsamples
vec_gmeans_scaled_train   = np.sqrt(tpr_scaled_train * (1-fpr_scaled_train))
vec_gmeans_scaled_test    = np.sqrt(tpr_scaled_test * (1-fpr_scaled_test))

# locate the index of the largest g-mean
ix_scaled_train = np.argmax(vec_gmeans_scaled_train)
ix_scaled_test  = np.argmax(vec_gmeans_scaled_test)

# Summarize the results
print('Training (Scaled features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_scaled_train[ix_scaled_train], vec_gmeans_scaled_train[ix_scaled_train]))
print(100*'-')
print('Testing (Scaled features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_scaled_test[ix_scaled_test], vec_gmeans_scaled_test[ix_scaled_test]))
print(100*'-')

# Plot the ROC curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [0, 1], 'b--')
plt.plot(fpr_scaled_train, tpr_scaled_train, 'r-', label='Linear SVC (Training), auc='+str(round(auc_scaled_train,6)))
plt.plot(fpr_scaled_test, tpr_scaled_test, 'g-', label='Linear SVC (Testing), auc='+str(round(auc_scaled_test,6)))
plt.scatter(fpr_scaled_train[ix_scaled_train], tpr_scaled_train[ix_scaled_train], marker='o', color='red', s=150)
plt.scatter(fpr_scaled_test[ix_scaled_test], tpr_scaled_test[ix_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=4, facecolor='white')
plt.xlabel('False Positive Rate', size=24)
plt.ylabel('True Positive Rate', size=24)
plt.title('Linear SVC ROC Curve: Scaled Features', size=24)
plt.show()

model_summary_svc = {'Model name': 'Linear SVC', 'AUC Train': auc_scaled_train, 'AUC Test': auc_scaled_test}

# Calculate the precision and recall for different values of threshold in the training and testing subsamples
precision_scaled_train, recall_scaled_train, threshold_scaled_train = metrics.precision_recall_curve(y_scaled_train, y_scaled_pred_proba_train)
average_precision_scaled_train = metrics.average_precision_score(y_scaled_train, y_scaled_pred_train)

precision_scaled_test, recall_scaled_test, threshold_scaled_test = metrics.precision_recall_curve(y_scaled_test, y_scaled_pred_proba_test)
average_precision_scaled_test = metrics.average_precision_score(y_scaled_test, y_scaled_pred_test)

# Calculate the F1 score
vec_f1_scaled_train = 2 * precision_scaled_train * recall_scaled_train / (precision_scaled_train + recall_scaled_train)
vec_f1_scaled_test  = 2 * precision_scaled_test * recall_scaled_test / (precision_scaled_test + recall_scaled_test)

# locate the index of the largest F1 score
ix_f1_scaled_train = np.argmax(vec_f1_scaled_train)
ix_f1_scaled_test  = np.argmax(vec_f1_scaled_test)

# Summarize the results
print('Training (Scaled features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_scaled_train[ix_f1_scaled_train], vec_f1_scaled_train[ix_f1_scaled_train]))
print(100*'-')
print('Testing (Scaled features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_scaled_test[ix_f1_scaled_test], vec_f1_scaled_test[ix_f1_scaled_test]))
print(100*'-')

# Calculate benchmark values for the training and testing subsamples
nopower_scaled_train = len(y_scaled_train[y_scaled_train==1]) / len(y_scaled_train)
nopower_scaled_test = len(y_scaled_test[y_scaled_test==1]) / len(y_scaled_test)

# Plot the PR curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [nopower_scaled_train, nopower_scaled_train], 'b--')
plt.plot([0, 1], [nopower_scaled_test, nopower_scaled_test], 'k--')
plt.plot(recall_scaled_train, precision_scaled_train, 'r-', label='Linear SVC (Training), average precision='+str(round(average_precision_scaled_train,6)))
plt.plot(recall_scaled_test, precision_scaled_test, 'g-', label='Linear SVC (Testing), average_precision='+str(round(average_precision_scaled_test,6)))
plt.scatter(recall_scaled_train[ix_f1_scaled_train], precision_scaled_train[ix_f1_scaled_train], marker='o', color='red', s =150)
plt.scatter(recall_scaled_test[ix_f1_scaled_test], precision_scaled_test[ix_f1_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=1, facecolor='white')
plt.xlabel('Recall', size=24)
plt.ylabel('Precision', size=24)
plt.title('Linear SVC PR Curve: Scaled Features', size=24)
plt.show()

model_summary_svc['F1 Train'] = vec_f1_scaled_train[ix_f1_scaled_train]
model_summary_svc['F1 Test'] = vec_f1_scaled_test[ix_f1_scaled_test]
summary = summary.append(model_summary_svc, ignore_index=True)

#we proceed with  K-Nearest Neighbors#
# Initialize error vectors
error_train = []
error_test  = []

# Define lower and upper bounds for k and loop over all adimissible values
k_low  = 1
k_high = 10

for k in range(k_low,k_high):
    print(k)
    # Create an instance of the KNN model
    knn_model = KNeighborsClassifier(n_neighbors=k)
    
    # Training sample
    knn_model.fit(X_scaled_train,y_scaled_train)
    y_scaled_pred_train = knn_model.predict(X_scaled_train)
    error_train.append(np.mean(y_scaled_train != y_scaled_pred_train))
    
    # Testing sample
    y_scaled_pred_test = knn_model.predict(X_scaled_test)
    error_test.append(np.mean(y_scaled_test!= y_scaled_pred_test))
    
# Plot the error curves
plt.figure(figsize=(24,16))
plt.plot(range(k_low,k_high),error_train,label='Train')
plt.plot(range(k_low,k_high),error_test,label='Test')
plt.xlabel('k-value', size=24)
plt.ylabel('Error', size=24)
plt.title('KNN Error Curve: Scaled Features', size=24)
plt.legend()
plt.show()

# Define the cross-validation approach
n_cv_knn = 5

# Define the maximum number of iterations
n_max_iter = 2000

# Create an instance of the class LinearSVC
knn_model = KNeighborsClassifier()

# Define the search space for the hyperparameters
search_space = dict()
search_space['n_neighbors'] = [2, 4, 6, 8, 10]
search_space['metric']      = ['euclidean', 'manhattan']

# Create an instance of the class GridSearchCV with desired characteristics
knn_grid_search_cv_scaled = GridSearchCV(estimator=knn_model, 
                                         param_grid=search_space, 
                                         scoring='accuracy', 
                                         cv=n_cv_knn, 
                                         n_jobs=-1)

# Run the search algorithm and summarize resuts 
knn_grid_search_cv_scaled.fit(X_scaled_train, y_scaled_train)

print(f'Best score: {knn_grid_search_cv_scaled.best_score_:.4f}')
print('Best hyperparameters: %s' % knn_grid_search_cv_scaled.best_params_)

# Create an instance of the class LinearSVC with specific selection of hyperparameters
knn_ccd_scaled = KNeighborsClassifier(n_neighbors=knn_grid_search_cv_scaled.best_params_['n_neighbors'], 
                                      metric=knn_grid_search_cv_scaled.best_params_['metric'])

# Fit the model
knn_ccd_scaled.fit(X_scaled_train, y_scaled_train)

# Predict outcomes in the testing subsample and calculate accuracy
y_scaled_pred_test = knn_ccd_scaled.predict(X_scaled_test)
print(f'Accuracy: {metrics.accuracy_score(y_scaled_pred_test,y_scaled_test):.4f}')

# Display cross-validation scores for the testing subsample
print(classification_report(y_scaled_test, y_scaled_pred_test))

# Estimate probabilities, and calcualate relevant ROC/AUC scores 
knn_calibrated_classifier_cv_ccd_scaled = CalibratedClassifierCV(knn_ccd_scaled) 
knn_calibrated_classifier_cv_ccd_scaled.fit(X_scaled_test, y_scaled_test)
y_scaled_pred_proba_test  = knn_calibrated_classifier_cv_ccd_scaled.predict_proba(X_scaled_test)[::,1]

fpr_scaled_test, tpr_scaled_test, threshold_scaled_test = metrics.roc_curve(y_scaled_test, y_scaled_pred_proba_test)
auc_scaled_test = metrics.roc_auc_score(y_scaled_test, y_scaled_pred_proba_test)

# Calculate the geometric mean for each threshold for the testing subsample
vec_gmeans_scaled_test    = np.sqrt(tpr_scaled_test * (1-fpr_scaled_test))

# locate the index of the largest g-mean
ix_scaled_test  = np.argmax(vec_gmeans_scaled_test)

# Summarize the results
print('Testing (Scaled features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_scaled_test[ix_scaled_test], vec_gmeans_scaled_test[ix_scaled_test]))
print(100*'-')

# Plot the ROC curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [0, 1], 'b--')
plt.plot(fpr_scaled_test, tpr_scaled_test, 'g-', label='Linear SVC (Testing), auc='+str(round(auc_scaled_test,6)))
plt.scatter(fpr_scaled_test[ix_scaled_test], tpr_scaled_test[ix_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=4, facecolor='white')
plt.xlabel('False Positive Rate', size=24)
plt.ylabel('True Positive Rate', size=24)
plt.title('KNN ROC Curve: Scaled Features', size=24)
plt.show()

model_summary_knn = {'Model name': 'KNN', 'AUC Train': None, 'AUC Test': auc_scaled_test}

# Calculate the precision and recall for different values of threshold in the training and testing subsamples
precision_scaled_test, recall_scaled_test, threshold_scaled_test = metrics.precision_recall_curve(y_scaled_test, y_scaled_pred_proba_test)
average_precision_scaled_test = metrics.average_precision_score(y_scaled_test, y_scaled_pred_test)

# Calculate the F1 score
vec_f1_scaled_test  = 2 * precision_scaled_test * recall_scaled_test / (precision_scaled_test + recall_scaled_test)

# locate the index of the largest F1 score
ix_f1_scaled_test  = np.argmax(vec_f1_scaled_test)

# Summarize the results
print('Testing (Scaled features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_scaled_test[ix_f1_scaled_test], vec_f1_scaled_test[ix_f1_scaled_test]))
print(100*'-')

# Calculate benchmark values for the training and testing subsamples
nopower_scaled_train = len(y_scaled_train[y_scaled_train==1]) / len(y_scaled_train)
nopower_scaled_test = len(y_scaled_test[y_scaled_test==1]) / len(y_scaled_test)

# Plot the PR curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [nopower_scaled_test, nopower_scaled_test], 'k--')
plt.plot(recall_scaled_test, precision_scaled_test, 'g-', label='Linear SVC (Testing), average_precision='+str(round(average_precision_scaled_test,6)))
plt.scatter(recall_scaled_test[ix_f1_scaled_test], precision_scaled_test[ix_f1_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=1, facecolor='white')
plt.xlabel('Recall', size=24)
plt.ylabel('Precision', size=24)
plt.title('Linear SVC PR Curve: Scaled Features', size=24)
plt.show()

model_summary_knn['F1 Test'] = vec_f1_scaled_test[ix_f1_scaled_test]
summary = summary.append(model_summary_knn, ignore_index=True)

#Descision Tree#
# Define inputs for DecisionTreeClassifier
n_max_depth = 5

# # Create an instance of the class DecisionTreeClassifier and fit a model on the training subsample
dt_model = DecisionTreeClassifier(max_depth=n_max_depth)
dt_model.fit(X_scaled_train,y_scaled_train)

# Plot the decision tree
plt.figure(figsize=(24,16))
plot_tree(dt_model, filled=True, feature_names=X_scaled_train.columns, class_names=['Non-default', 'Default'])
plt.show()

# Define the cross-validation approach
n_cv_dt = 5

# Create an instance of the class DecisionTreeClassifier
dt_model = DecisionTreeClassifier()

# Define the search space for the hyperparameters
search_space = dict()
search_space['max_depth']        = [2, 3, 4, 5, 6, 7]
search_space['max_features']     = [2, 3, 4, 5, 6, 7]
search_space['min_samples_leaf'] = [2, 3, 4, 5, 6, 7]
search_space['criterion']        = ['gini', 'entropy']

# Create an instance of the class RandomizedSearchCV with desired characteristics
dt_randomized_search_cv_scaled = RandomizedSearchCV(estimator=dt_model, 
                                                    param_distributions=search_space, 
                                                    scoring='accuracy', 
                                                    cv=n_cv_dt, 
                                                    n_jobs=-1)

# Run the search algorithm and summarize resuts 
dt_randomized_search_cv_scaled.fit(X_scaled_train, y_scaled_train)

print(f'Best score: {dt_randomized_search_cv_scaled.best_score_:.4f}')
print('Best hyperparameters: %s' % dt_randomized_search_cv_scaled.best_params_)

# Create an instance of the class DecisionTreeClassifier with specific selection of hyperparameters
dt_ccd_scaled = DecisionTreeClassifier(max_depth=dt_randomized_search_cv_scaled.best_params_['max_depth'], 
                                       max_features=dt_randomized_search_cv_scaled.best_params_['max_features'], 
                                       min_samples_leaf=dt_randomized_search_cv_scaled.best_params_['min_samples_leaf'],
                                       criterion=dt_randomized_search_cv_scaled.best_params_['criterion'])

# Fit the model
dt_ccd_scaled.fit(X_scaled_train, y_scaled_train)

# Predict outcomes in the training and testing subsamples and calculate accuracy
y_scaled_pred_train = dt_ccd_scaled.predict(X_scaled_train)
print(f'Accuracy: {metrics.accuracy_score(y_scaled_pred_train,y_scaled_train):.4f}')

y_scaled_pred_test = dt_ccd_scaled.predict(X_scaled_test)
print(f'Accuracy: {metrics.accuracy_score(y_scaled_pred_test,y_scaled_test):.4f}')

# Calculate and display cross-validation scores for the training and testing subsamples
dt_cv_scores_scaled =cross_val_score(dt_ccd_scaled, X_scaled_train, y_scaled_train, cv=n_cv_dt)
print(f'Average CV score (+/- one standard deviation): {dt_cv_scores_scaled.mean():.4f} +/- {dt_cv_scores_scaled.std():.4f}')

print(classification_report(y_scaled_train, y_scaled_pred_train))
print(classification_report(y_scaled_test, y_scaled_pred_test))

# Estimate probabilities, and calcualate relevant ROC/AUC scores 
dt_calibrated_classifier_cv_ccd_scaled = CalibratedClassifierCV(dt_ccd_scaled) 
dt_calibrated_classifier_cv_ccd_scaled.fit(X_scaled_train, y_scaled_train)
y_scaled_pred_proba_train = dt_calibrated_classifier_cv_ccd_scaled.predict_proba(X_scaled_train)[::,1]

fpr_scaled_train, tpr_scaled_train, threshold_scaled_train  = metrics.roc_curve(y_scaled_train, y_scaled_pred_proba_train)
auc_scaled_train          = metrics.roc_auc_score(y_scaled_train, y_scaled_pred_proba_train)

dt_calibrated_classifier_cv_ccd_scaled.fit(X_scaled_test, y_scaled_test)
y_scaled_pred_proba_test  = dt_calibrated_classifier_cv_ccd_scaled.predict_proba(X_scaled_test)[::,1]
fpr_scaled_test, tpr_scaled_test, threshold_scaled_test = metrics.roc_curve(y_scaled_test, y_scaled_pred_proba_test)
auc_scaled_test           = metrics.roc_auc_score(y_scaled_test, y_scaled_pred_proba_test)

# Calculate the geometric mean for each threshold for the training and testing subsamples
vec_gmeans_scaled_train   = np.sqrt(tpr_scaled_train * (1-fpr_scaled_train))
vec_gmeans_scaled_test    = np.sqrt(tpr_scaled_test * (1-fpr_scaled_test))

# locate the index of the largest g-mean
ix_scaled_train = np.argmax(vec_gmeans_scaled_train)
ix_scaled_test  = np.argmax(vec_gmeans_scaled_test)

# Summarize the results
print('Training (Scaled features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_scaled_train[ix_scaled_train], vec_gmeans_scaled_train[ix_scaled_train]))
print(100*'-')
print('Testing (Scaled features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_scaled_test[ix_scaled_test], vec_gmeans_scaled_test[ix_scaled_test]))
print(100*'-')

# Plot the ROC curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [0, 1], 'b--')
plt.plot(fpr_scaled_train, tpr_scaled_train, 'r-', label='Decision Tree (Training), auc='+str(round(auc_scaled_train,6)))
plt.plot(fpr_scaled_test, tpr_scaled_test, 'g-', label='Decision Tree (Testing), auc='+str(round(auc_scaled_test,6)))
plt.scatter(fpr_scaled_train[ix_scaled_train], tpr_scaled_train[ix_scaled_train], marker='o', color='red', s=150)
plt.scatter(fpr_scaled_test[ix_scaled_test], tpr_scaled_test[ix_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=4, facecolor='white')
plt.xlabel('False Positive Rate', size=24)
plt.ylabel('True Positive Rate', size=24)
plt.title('Decision Tree ROC Curve: Scaled Features', size=24)
plt.show()

model_summary_dt = {'Model name': 'Decision Tree', 'AUC Train': auc_scaled_train, 'AUC Test': auc_scaled_test}

# Calculate the precision and recall for different values of threshold in the training and testing subsamples
precision_scaled_train, recall_scaled_train, threshold_scaled_train = metrics.precision_recall_curve(y_scaled_train, y_scaled_pred_proba_train)
average_precision_scaled_train = metrics.average_precision_score(y_scaled_train, y_scaled_pred_train)

precision_scaled_test, recall_scaled_test, threshold_scaled_test = metrics.precision_recall_curve(y_scaled_test, y_scaled_pred_proba_test)
average_precision_scaled_test = metrics.average_precision_score(y_scaled_test, y_scaled_pred_test)

# Calculate the F1 score
vec_f1_scaled_train = 2 * precision_scaled_train * recall_scaled_train / (precision_scaled_train + recall_scaled_train)
vec_f1_scaled_test  = 2 * precision_scaled_test * recall_scaled_test / (precision_scaled_test + recall_scaled_test)

# locate the index of the largest F1 score
ix_f1_scaled_train = np.argmax(vec_f1_scaled_train)
ix_f1_scaled_test  = np.argmax(vec_f1_scaled_test)

# Summarize the results
print('Training (Scaled features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_scaled_train[ix_f1_scaled_train], vec_f1_scaled_train[ix_f1_scaled_train]))
print(100*'-')
print('Testing (Scaled features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_scaled_test[ix_f1_scaled_test], vec_f1_scaled_test[ix_f1_scaled_test]))
print(100*'-')

# Calculate benchmark values for the training and testing subsamples
nopower_scaled_train = len(y_scaled_train[y_scaled_train==1]) / len(y_scaled_train)
nopower_scaled_test = len(y_scaled_test[y_scaled_test==1]) / len(y_scaled_test)

# Plot the PR curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [nopower_scaled_train, nopower_scaled_train], 'b--')
plt.plot([0, 1], [nopower_scaled_test, nopower_scaled_test], 'k--')
plt.plot(recall_scaled_train, precision_scaled_train, 'r-', label='Decision Tree (Training), average precision='+str(round(average_precision_scaled_train,6)))
plt.plot(recall_scaled_test, precision_scaled_test, 'g-', label='Decision Tree (Testing), average_precision='+str(round(average_precision_scaled_test,6)))
plt.scatter(recall_scaled_train[ix_f1_scaled_train], precision_scaled_train[ix_f1_scaled_train], marker='o', color='red', s =150)
plt.scatter(recall_scaled_test[ix_f1_scaled_test], precision_scaled_test[ix_f1_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=1, facecolor='white')
plt.xlabel('Recall', size=24)
plt.ylabel('Precision', size=24)
plt.title('Decision Tree PR Curve: Scaled Features', size=24)
plt.show()

model_summary_dt['F1 Train'] = vec_f1_scaled_train[ix_f1_scaled_train]
model_summary_dt['F1 Test'] = vec_f1_scaled_test[ix_f1_scaled_test]
summary = summary.append(model_summary_dt, ignore_index=True)

#Random Forest#
# Define the cross-validation approach
n_cv_rf = 5

# Create an instance of the class RandomForestClassifier
rf_model = RandomForestClassifier()

# Define the search space for the hyperparameters
search_space = dict()
search_space['n_estimators']     = [50, 100, 150, 200, 250]
search_space['max_depth']        = [2, 3, 4, 5, 6, 7]
search_space['max_features']     = [2, 3, 4, 5, 6, 7]
search_space['min_samples_leaf'] = [2, 3, 4, 5, 6, 7]
search_space['criterion']        = ['gini', 'entropy']

# Create an instance of the class RandomizedSearchCV with desired characteristics
rf_randomized_search_cv_scaled = RandomizedSearchCV(estimator=rf_model, 
                                                    param_distributions=search_space, 
                                                    scoring='accuracy', 
                                                    cv=n_cv_rf, 
                                                    n_jobs=-1)

# Run the search algorithm and summarize resuts 
rf_randomized_search_cv_scaled.fit(X_scaled_train, y_scaled_train)

print(f'Best score: {rf_randomized_search_cv_scaled.best_score_:.4f}')
print('Best hyperparameters: %s' % rf_randomized_search_cv_scaled.best_params_)

# Create an instance of the class RandomForestClassifier with specific selection of hyperparameters
rf_ccd_scaled = RandomForestClassifier(n_estimators=rf_randomized_search_cv_scaled.best_params_['n_estimators'], 
                                       max_depth=rf_randomized_search_cv_scaled.best_params_['max_depth'], 
                                       max_features=rf_randomized_search_cv_scaled.best_params_['max_features'], 
                                       min_samples_leaf=rf_randomized_search_cv_scaled.best_params_['min_samples_leaf'],
                                       criterion=rf_randomized_search_cv_scaled.best_params_['criterion'])

# Fit the model
rf_ccd_scaled.fit(X_scaled_train, y_scaled_train)

# Predict outcomes in the training and testing subsamples and calculate accuracy
y_scaled_pred_train = rf_ccd_scaled.predict(X_scaled_train)
print(f'Accuracy: {metrics.accuracy_score(y_scaled_pred_train,y_scaled_train):.4f}')

y_scaled_pred_test = rf_ccd_scaled.predict(X_scaled_test)
print(f'Accuracy: {metrics.accuracy_score(y_scaled_pred_test,y_scaled_test):.4f}')

# Calculate and display cross-validation scores for the training and testing subsamples
rf_cv_scores_scaled =cross_val_score(rf_ccd_scaled, X_scaled_train, y_scaled_train, cv=n_cv_rf)
print(f'Average CV score (+/- one standard deviation): {rf_cv_scores_scaled.mean():.4f} +/- {rf_cv_scores_scaled.std():.4f}')

print(classification_report(y_scaled_train, y_scaled_pred_train))
print(classification_report(y_scaled_test, y_scaled_pred_test))

# Extract importances
importances = rf_ccd_scaled.feature_importances_
std         = np.std([tree.feature_importances_ for tree in rf_ccd_scaled.estimators_],
                     axis=0)

ix_importances = np.argsort(importances)[::-1]

# Plot the feature importances for the random forest
plt.figure(figsize=(24, 16))
plt.bar(range(X_scaled_train.shape[1]), importances[ix_importances],
       color='b', yerr=std[ix_importances], align='center')
plt.xticks(range(X_scaled_train.shape[1]), X_scaled_train.columns[ix_importances],rotation=90)
plt.xlim([-1, X_scaled_train.shape[1]])
plt.title('Random Forest Feature Importances', size=24)
plt.show()

# Estimate probabilities, and calcualate relevant ROC/AUC scores 
rf_calibrated_classifier_cv_ccd_scaled = CalibratedClassifierCV(dt_ccd_scaled) 
rf_calibrated_classifier_cv_ccd_scaled.fit(X_scaled_train, y_scaled_train)
y_scaled_pred_proba_train = rf_calibrated_classifier_cv_ccd_scaled.predict_proba(X_scaled_train)[::,1]

fpr_scaled_train, tpr_scaled_train, threshold_scaled_train  = metrics.roc_curve(y_scaled_train, y_scaled_pred_proba_train)
auc_scaled_train          = metrics.roc_auc_score(y_scaled_train, y_scaled_pred_proba_train)

rf_calibrated_classifier_cv_ccd_scaled.fit(X_scaled_test, y_scaled_test)
y_scaled_pred_proba_test  = rf_calibrated_classifier_cv_ccd_scaled.predict_proba(X_scaled_test)[::,1]
fpr_scaled_test, tpr_scaled_test, threshold_scaled_test = metrics.roc_curve(y_scaled_test, y_scaled_pred_proba_test)
auc_scaled_test           = metrics.roc_auc_score(y_scaled_test, y_scaled_pred_proba_test)

# Calculate the geometric mean for each threshold for the training and testing subsamples
vec_gmeans_scaled_train   = np.sqrt(tpr_scaled_train * (1-fpr_scaled_train))
vec_gmeans_scaled_test    = np.sqrt(tpr_scaled_test * (1-fpr_scaled_test))

# locate the index of the largest g-mean
ix_scaled_train = np.argmax(vec_gmeans_scaled_train)
ix_scaled_test  = np.argmax(vec_gmeans_scaled_test)

# Summarize the results
print('Training (Scaled features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_scaled_train[ix_scaled_train], vec_gmeans_scaled_train[ix_scaled_train]))
print(100*'-')
print('Testing (Scaled features):')
print('Best threshold = %f, Geometric mean = %.3f' % (threshold_scaled_test[ix_scaled_test], vec_gmeans_scaled_test[ix_scaled_test]))
print(100*'-')

# Plot the ROC curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [0, 1], 'b--')
plt.plot(fpr_scaled_train, tpr_scaled_train, 'r-', label='Random Forest (Training), auc='+str(round(auc_scaled_train,6)))
plt.plot(fpr_scaled_test, tpr_scaled_test, 'g-', label='Random Forest (Testing), auc='+str(round(auc_scaled_test,6)))
plt.scatter(fpr_scaled_train[ix_scaled_train], tpr_scaled_train[ix_scaled_train], marker='o', color='red', s=150)
plt.scatter(fpr_scaled_test[ix_scaled_test], tpr_scaled_test[ix_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=4, facecolor='white')
plt.xlabel('False Positive Rate', size=24)
plt.ylabel('True Positive Rate', size=24)
plt.title('Random Forest ROC Curve: Scaled Features', size=24)
plt.show()

model_summary_rf = {'Model name': 'Random Forest', 'AUC Train': auc_scaled_train, 'AUC Test': auc_scaled_test}

# Calculate the precision and recall for different values of threshold in the training and testing subsamples
precision_scaled_train, recall_scaled_train, threshold_scaled_train = metrics.precision_recall_curve(y_scaled_train, y_scaled_pred_proba_train)
average_precision_scaled_train = metrics.average_precision_score(y_scaled_train, y_scaled_pred_train)

precision_scaled_test, recall_scaled_test, threshold_scaled_test = metrics.precision_recall_curve(y_scaled_test, y_scaled_pred_proba_test)
average_precision_scaled_test = metrics.average_precision_score(y_scaled_test, y_scaled_pred_test)

# Calculate the F1 score
vec_f1_scaled_train = 2 * precision_scaled_train * recall_scaled_train / (precision_scaled_train + recall_scaled_train)
vec_f1_scaled_test  = 2 * precision_scaled_test * recall_scaled_test / (precision_scaled_test + recall_scaled_test)

# locate the index of the largest F1 score
ix_f1_scaled_train = np.argmax(vec_f1_scaled_train)
ix_f1_scaled_test  = np.argmax(vec_f1_scaled_test)

# Summarize the results
print('Training (Scaled features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_scaled_train[ix_f1_scaled_train], vec_f1_scaled_train[ix_f1_scaled_train]))
print(100*'-')
print('Testing (Scaled features):')
print('Best threshold = %f, F1 score = %.3f' % (threshold_scaled_test[ix_f1_scaled_test], vec_f1_scaled_test[ix_f1_scaled_test]))
print(100*'-')

# Calculate benchmark values for the training and testing subsamples
nopower_scaled_train = len(y_scaled_train[y_scaled_train==1]) / len(y_scaled_train)
nopower_scaled_test = len(y_scaled_test[y_scaled_test==1]) / len(y_scaled_test)

# Plot the PR curve
plt.figure(figsize=(24,16))
plt.plot([0, 1], [nopower_scaled_train, nopower_scaled_train], 'b--')
plt.plot([0, 1], [nopower_scaled_test, nopower_scaled_test], 'k--')
plt.plot(recall_scaled_train, precision_scaled_train, 'r-', label='Random Forest (Training), average precision='+str(round(average_precision_scaled_train,6)))
plt.plot(recall_scaled_test, precision_scaled_test, 'g-', label='Random Forest (Testing), average_precision='+str(round(average_precision_scaled_test,6)))
plt.scatter(recall_scaled_train[ix_f1_scaled_train], precision_scaled_train[ix_f1_scaled_train], marker='o', color='red', s =150)
plt.scatter(recall_scaled_test[ix_f1_scaled_test], precision_scaled_test[ix_f1_scaled_test], marker='o', color='green', s=150)
plt.legend(loc=1, facecolor='white')
plt.xlabel('Recall', size=24)
plt.ylabel('Precision', size=24)
plt.title('Random Forest PR Curve: Scaled Features', size=24)
plt.show()

model_summary_rf['F1 Train'] = vec_f1_scaled_train[ix_f1_scaled_train]
model_summary_rf['F1 Test'] = vec_f1_scaled_test[ix_f1_scaled_test]
summary = summary.append(model_summary_rf, ignore_index=True)

summary #Logistic Regression - log regression with standardized features#

#the KNN gives the best rezults for AUC test and F1 but has NaN at auc train and f1 train field
#the overall best models apear Random forest and Decision tree with minute diffrences#

"""#**3**#"""

# Import / install relevant Python packages
import numpy as np
from numpy import (where, unique)
import pandas as pd
from pandas import set_option
import datetime as dt
import statsmodels.api as sm
from scipy.stats import (randint, loguniform, multivariate_normal)

from collections import Counter

from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV, cross_val_score)

from sklearn.datasets import (make_blobs, make_classification)
from sklearn.metrics import silhouette_samples
from sklearn.cluster import (KMeans, MiniBatchKMeans, AgglomerativeClustering, DBSCAN)
from sklearn.mixture import GaussianMixture
from sklearn.neighbors import NearestNeighbors

from scipy.cluster.hierarchy import (linkage, dendrogram, set_link_color_palette)

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
plt.style.use('ggplot')
import seaborn as sns

data['X6']

data['X12']

X.index

#lets check out the data on a graph#

plt.figure(figsize=(24,16))
plt.scatter(X.index, 
            X['X12'],
            s=150, 
            edgecolor = 'k',
            alpha=0.7)

plt.title('ATM1', size=24)
plt.xlabel('INDEX', size=24)
plt.ylabel('ATM1', size=24)
plt.xticks(size=24) 
plt.yticks(size=24)   
plt.show()

#there seems to be some inconcistencies amount of bill statement should not be negative be negative#

plt.figure(figsize=(24,16))
plt.scatter(X.index, 
            X['X6'],
            s=150, 
            edgecolor = 'k',
            alpha=0.7)

plt.title('PAY0', size=24)
plt.xlabel('INDEX', size=24)
plt.ylabel('PAY0', size=24)
plt.xticks(size=24) 
plt.yticks(size=24)   
plt.show()

#A negative balance indicates that your bill was overpaid and that you may be eligible for a refund.#
#we should not take these records in account#

#data cleanup#
data_for_clustering=X[np.logical_or(X['X6']>0,X['X12']>0)]
#We will proceed to standardize the data#
data_for_clustering=(data_for_clustering - data_for_clustering.mean()) / (data_for_clustering.std())

# Create a scatter plot
plt.figure(figsize=(24,16))
plt.scatter(data_for_clustering['X6'], 
            data_for_clustering['X12'],
            s=150, 
            edgecolor = 'k',
            alpha=0.7)

plt.title('PAY vs ATM1 scater plot', size=24)
plt.xlabel('atm1_stand', size=24)
plt.ylabel('PAY0_stand', size=24)
plt.xticks(size=24) 
plt.yticks(size=24)   
plt.show()

#1 K-means Clustering#
# Define the bounds for the number of clusters
n_clusters_lb = 1
n_clusters_ub = 11

# Initiate the cluster inertia (WCSS) vector 
kmeans_interia = []

# Fit K-means model for different numbers of clusters
for ix_clusters in range(n_clusters_lb, n_clusters_ub):
    kmeans_model = KMeans(n_clusters=ix_clusters,
                          init='k-means++',
                          n_init=10,
                          max_iter=1000,
                          random_state=1)
    kmeans_model.fit(data_for_clustering[['X6','X12']])
    kmeans_interia.append(kmeans_model.inertia_)

# Create the elbow plot
plt.figure(figsize=(16,8))    
plt.plot(range(n_clusters_lb,n_clusters_ub), kmeans_interia, marker='s')
plt.title('K-Means Clustering: Elbow Plot', size=16)
plt.xlabel('Number of clusters', size=16)
plt.ylabel('Cluster Inertia (WCSS)', size=16)
plt.xticks(size=16) 
plt.yticks(size=16)  
plt.show()

#the curve levels of around 7 clusters#

# Define the key inputs for the K-means model silhoutte plot
n_clusters = 11
n_max_iter = 1000
n_init     = 10
init_type  = 'k-means++'

# Create an instance of the class KMeans, fit the model, and predict cluster memberships
kmeans_model = KMeans(n_clusters=n_clusters,
                      init=init_type,
                      n_init=n_init,
                      max_iter=n_max_iter,
                      random_state=1)
y_pred = kmeans_model.fit_predict(data_for_clustering[['X6','X12']])

# Retrieve and count unique clusters
kmeans_clusters = np.unique(y_pred)
n_clusters      = kmeans_clusters.shape[0]

# Calculate silhouette values for all samples
silhouette_vals = silhouette_samples(data_for_clustering[['X6','X12']], y_pred, metric='euclidean')

# Create the silhouette plot
plt.figure(figsize=(24,16)) 

y_ax_lower, y_ax_upper = 0, 0
yticks = []

for ix_clusters in kmeans_clusters:    
    c_silhouette_vals = silhouette_vals[y_pred == ix_clusters]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    plt.barh(range(y_ax_lower, y_ax_upper),
             c_silhouette_vals,
             height=0.7,
             edgecolor='none')
    yticks.append((y_ax_lower + y_ax_upper) / 2)
    y_ax_lower += len(c_silhouette_vals)
    
silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg,
            color='k',
            linestyle='--')
plt.yticks(yticks, kmeans_clusters + 1)
plt.xticks(size=24) 
plt.yticks(size=24)
plt.title('K-Means Clustering: Silhouette Plot', size=24)
plt.ylabel('Cluster', size=24)
plt.xlabel('Silhouette coefficient', size=24)
plt.show()

#thec silhoute coefficient should be as close to one as posible #

#it would seem that both methods agree on the optimal number of clusters that is 7#

# Define the avrage Silhouette Score
n_clusters_lb = 2
n_clusters_ub = 11

# Initiate the average silhouette score vector 
kmeans_silhouette_avg = []

# Fit K-means model for different numbers of clusters
for ix_clusters in range(n_clusters_lb, n_clusters_ub):
    kmeans_model = KMeans(n_clusters=ix_clusters,
                          init='k-means++',
                          n_init=10,
                          max_iter=1000,
                          random_state=1)
    y_pred = kmeans_model.fit_predict(data_for_clustering[['X6','X12']])
    silhouette_vals = silhouette_samples(data_for_clustering[['X6','X12']], y_pred, metric='euclidean')
    kmeans_silhouette_avg.append(np.mean(silhouette_vals))

# Create the elbow plot
plt.figure(figsize=(16,8))    
plt.plot(range(n_clusters_lb,n_clusters_ub), kmeans_silhouette_avg, marker='s')
plt.title('K-Means Clustering: Average Silhouette Score Plot', size=16)
plt.xlabel('Number of clusters', size=16)
plt.ylabel('Average silhouette score', size=16)
plt.xticks(size=16) 
plt.yticks(size=16)  
plt.show()

# Define the cross-validation approach
n_cv_kmeans = 5

# Define the maximum number of iterations
n_max_iter = 500
init_type  = 'k-means++'

# Create an instance of the class KMeans
kmeans_model = KMeans(init=init_type,
                      max_iter=n_max_iter)

# Define the search space for the hyperparameters
search_space = dict()
search_space['n_clusters'] = [2, 3, 4, 5, 6, 7, 8, 9, 10]
search_space['n_init']     = [10, 20, 30, 40, 50]

# Create an instance of the class GridSearchCV with desired characteristics
kmeans_grid_search_cv = GridSearchCV(estimator=kmeans_model, 
                                     param_grid=search_space, 
                                     scoring='adjusted_mutual_info_score', 
                                     cv=n_cv_kmeans, 
                                     n_jobs=-1)

# Run the search algorithm and summarize resuts 
kmeans_grid_search_cv.fit(data_for_clustering[['X6','X12']].to_numpy())

print(f'Best score: {kmeans_grid_search_cv.best_score_:.4f}')
print('Best hyperparameters: %s' % kmeans_grid_search_cv.best_params_)

#we will stay with the previous conclusion of 7#
#it seems the algoritham encounters some computational problems #

# Gaussian Mixture Models#

# Define the key inputs for the GMM
n_components = 11
n_init       = 10
random_state = 1

# Create an instance of the class GaussianMixture, fit the model, and predict the cluster membership
gmm_model = GaussianMixture(n_components=n_components,
                            n_init=n_init,
                            random_state=random_state)
y_pred = gmm_model.fit_predict(data_for_clustering[['X6','X12']])

# Retrieve and count unique clusters
gmm_clusters = np.unique(y_pred)
n_clusters   = gmm_clusters.shape[0]

# Calculate silhouette values for all samples
silhouette_vals = silhouette_samples(data_for_clustering[['X6','X12']], y_pred, metric='euclidean')

# Create the silhouette plot
plt.figure(figsize=(24,16)) 

y_ax_lower, y_ax_upper = 0, 0
yticks = []

for ix_clusters in gmm_clusters:    
    c_silhouette_vals = silhouette_vals[y_pred == ix_clusters]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    plt.barh(range(y_ax_lower, y_ax_upper),
             c_silhouette_vals,
             height=0.7,
             edgecolor='none')
    yticks.append((y_ax_lower + y_ax_upper) / 2)
    y_ax_lower += len(c_silhouette_vals)
    
silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg,
            color='k',
            linestyle='--')
plt.yticks(yticks, gmm_clusters + 1)
plt.xticks(size=24) 
plt.yticks(size=24)
plt.title('GMM Clustering: Silhouette Plot', size=24)
plt.ylabel('Cluster', size=24)
plt.xlabel('Silhouette coefficient', size=24)
plt.show()

#it seem that we are seing again 7 for the number of clusters but this time the GMM seems to give a much better silhoutte score#

# Define the bounds for the number of clusters
n_clusters_lb = 2
n_clusters_ub = 11

# Initiate the average silhouette score vector 
gmm_silhouette_avg = []

# Fit K-means model for different numbers of clusters
for ix_clusters in range(n_clusters_lb, n_clusters_ub):
    gmm_model = GaussianMixture(n_components=ix_clusters,
                                n_init=10,
                                random_state=1)
    y_pred = gmm_model.fit_predict(data_for_clustering[['X6','X12']])
    silhouette_vals = silhouette_samples(data_for_clustering[['X6','X12']], y_pred, metric='euclidean')
    gmm_silhouette_avg.append(np.mean(silhouette_vals))

# Create the elbow plot
plt.figure(figsize=(16,8))    
plt.plot(range(n_clusters_lb,n_clusters_ub), gmm_silhouette_avg, marker='s')
plt.title('GMM Clustering: Average Silhouette Score Plot', size=16)
plt.xlabel('Number of clusters', size=16)
plt.ylabel('Average silhouette score', size=16)
plt.xticks(size=16) 
plt.yticks(size=16)  
plt.show()

#the avrage score levels out at 7 with not much improvement after#

#AIC and BIC
n_clusters_lb = 1
n_clusters_ub = 11

# Initiate the vectors of AIC and BIC scores
gmm_aic = []
gmm_bic = []

# Fit GMM model for different numbers of clusters
for ix_clusters in range(n_clusters_lb, n_clusters_ub):
    gmm_model = GaussianMixture(n_components=ix_clusters,
                                n_init=10,
                                random_state=1)
    gmm_model.fit(data_for_clustering[['X6','X12']])
    
    gmm_aic.append(gmm_model.aic(data_for_clustering[['X6','X12']]))
    gmm_bic.append(gmm_model.bic(data_for_clustering[['X6','X12']]))

# Display the key information       
print(f'Best AIC score: {round(min(gmm_aic),2)}')
print(f'Optimal number of clusters: {gmm_aic.index(min(gmm_aic))+1}') 
print(100*'-')
print(f'Best BIC score: {round(min(gmm_bic),2)}')
print(f'Optimal number of clusters: {gmm_bic.index(min(gmm_bic))+1}') 
print(100*'-')
    
# Create the elbow plot
plt.figure(figsize=(16,8))    
plt.plot(range(n_clusters_lb,n_clusters_ub), gmm_aic, marker='s', label='AIC')
plt.plot(range(n_clusters_lb,n_clusters_ub), gmm_bic, marker='o', label='BIC')
plt.legend(fontsize=16, facecolor='white')
plt.title('GMM Clustering: AIC and BIC Scores', size=16)
plt.xlabel('Number of clusters', size=16)
plt.ylabel('SCore', size=16)
plt.xticks(size=16) 
plt.yticks(size=16)  
plt.show()

#here the algoritham sugests 10 but from the chart we can see that the curves level of after 7 clusters#

#3 Hierarchical Clustering#

# Define the key inputs for the AgglomerativeClustering model
distance_threshold = 0
n_clusters         = 11
linkage_type       = 'ward' 

# Create an instance of the class AgglomerativeClustering and fit the model
hca_model = AgglomerativeClustering(linkage=linkage_type, 
                                    distance_threshold=distance_threshold, 
                                    n_clusters=None)
hca_model.fit(data_for_clustering[['X6','X12']])

# Prepare inputs for the linkage matrix
counts = np.zeros(hca_model.children_.shape[0])
n_samples = len(hca_model.labels_)

for i, children in enumerate(hca_model.children_):
    current_count = 0
    for ix_child in children:
        if ix_child < n_samples:
            current_count += 1  
        else:
            current_count += counts[ix_child - n_samples]
    counts[i] = current_count

# Compute the linkage matrix    
linkage_matrix = np.column_stack([hca_model.children_, hca_model.distances_, counts]).astype(float)

# Calculate the distance threshold (cut-off level) given the number of clusters
distances_sorted = np.sort(linkage_matrix[:,2])
distances_sorted = distances_sorted[::-1]
n_color_threshold  = 0.5 * (distances_sorted[n_clusters-2] + distances_sorted[n_clusters-1])

# Specify the color palette
#set_link_color_palette(['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'])

# Create a dendogram plot (and add the threshold line)
plt.figure(figsize=(24,16))
dendrogram(linkage_matrix, truncate_mode='level', p=4)
#dendrogram(linkage_matrix, truncate_mode='level', p=4, color_threshold=n_color_threshold, above_threshold_color='k')
plt.axhline(n_color_threshold, color='orange', linestyle='--')
plt.title('Hierarchical Clustering Dendrogram (scikit-learn)', size=24)
plt.xlabel('Number of points in node (or index of point if no parenthesis)', size=24)
plt.ylabel('Dissimilarity measure', size=24)
plt.xticks(size=24) 
plt.yticks(size=24)   
plt.show()

# Reset the color palette to the default values
set_link_color_palette(None)

# Define the bounds for the number of clusters
n_clusters_lb = 2
n_clusters_ub = 11

# Initiate the average silhouette score vector 
hca_silhouette_avg = []

# Fit AgglomerativeClustering model for different numbers of clusters
for ix_clusters in range(n_clusters_lb, n_clusters_ub):
    hca_model = AgglomerativeClustering(n_clusters=ix_clusters,
                                        linkage='ward')
    y_pred = hca_model.fit_predict(data_for_clustering[['X6','X12']])
    silhouette_vals = silhouette_samples(data_for_clustering[['X6','X12']], y_pred, metric='euclidean')
    hca_silhouette_avg.append(np.mean(silhouette_vals))

# Create the elbow plot
plt.figure(figsize=(16,8))    
plt.plot(range(n_clusters_lb,n_clusters_ub), hca_silhouette_avg, marker='s')
plt.title('Hierarchical Agglomerative Clustering: Average Silhouette Score Plot', size=16)
plt.xlabel('Number of clusters', size=16)
plt.ylabel('Average silhouette score', size=16)
plt.xticks(size=16) 
plt.yticks(size=16)  
plt.show()

#Hierarchical Agglomerative Clustering method is sugesting 8 clusters#

#DBSCAN#

# Define the bounds for the number of clusters for the entire set (goes on without end)
eps_lb   = 0.3
eps_ub   = 0.6
eps_step = 0.01

min_samples_lb   = 5
min_samples_ub   = 25
min_samples_step = 1

# Fit DBSCAN model for different eps and min_samples values
dbscan_output = []

for ix_eps in np.arange(eps_lb, eps_ub, eps_step):
    dbscan_silhouette_avg_vec = []
    
    for ix_min_samples in np.arange(min_samples_lb, min_samples_ub, min_samples_step):
        dbscan_model = DBSCAN(eps=ix_eps,
                              min_samples=ix_min_samples)
        y_pred = dbscan_model.fit_predict(data_for_clustering[['X6','X12']])
        
        dbscan_silhouette_vals = silhouette_samples(data_for_clustering[['X6','X12']], y_pred, metric='euclidean')
        dbscan_silhouette_avg_vec.append(np.mean(dbscan_silhouette_vals))
        
        dbscan_output.append((ix_min_samples, ix_eps, np.mean(dbscan_silhouette_vals)))
        
    dbscan_silhouette_avg_vec = np.array(dbscan_silhouette_avg_vec)
    
    if ix_eps == eps_lb:
        dbscan_silhouette_avg_mat = dbscan_silhouette_avg_vec
    else:
        dbscan_silhouette_avg_mat = np.vstack((dbscan_silhouette_avg_mat, dbscan_silhouette_avg_vec))

# Display the key information       
optimal_min_samples, optimal_eps, max_silhouette_avg = sorted(dbscan_output, key=lambda x:x[-1])[-1]
print(f'Best silhouette_score: {round(max_silhouette_avg,2)}')
print(f'Optimal min_samples: {optimal_min_samples}')
print(f'Optimal eps: {round(optimal_eps,2)}')  

labels = DBSCAN(min_samples=optimal_min_samples, eps = optimal_eps).fit(data_for_clustering[['X6','X12']]).labels_
clusters = len(Counter(labels))
print(f'Number of clusters: {clusters}')
print(f'Number of outliers: {Counter(labels)[-1]}')
print(100*'-')

# Plot distances
plt.figure(figsize=(16,12))

xticklabels=np.arange(min_samples_lb, min_samples_ub, min_samples_step)
yticklabels=np.around(np.arange(eps_lb, eps_ub, eps_step),2)

dbscan_heatmap = sns.heatmap(dbscan_silhouette_avg_mat, xticklabels=xticklabels, yticklabels=yticklabels)
plt.title('DBSCAN Heatmap of Average Silhouette Scores', size=16)
plt.xlabel('Minimum cluster size', size=16)
plt.ylabel('Epsilon radius', size=16)
plt.xticks(size=16) 
plt.yticks(size=16) 

dbscan_heatmap.set_yticklabels(dbscan_heatmap.get_yticklabels(), rotation=0)

cbar = dbscan_heatmap.collections[0].colorbar
cbar.ax.tick_params(labelsize=16)

plt.show()

#Results#
#K-means:the silhoutte plot and elbow plot sugests seven the average silhoute score sugests 8 
#GMM: the silhoutte plot and average silhoute score sugests seven from the graph that minimizes the AIC and BIC it too levels of at seven
#Hierarchical Clustering: average silhoute score sugests eight
#DBSCAN:for the best silhouette score we get eight as the optimal number of clusters

#conclusion:based on the results we recommend 8 as the optimal number of clusters